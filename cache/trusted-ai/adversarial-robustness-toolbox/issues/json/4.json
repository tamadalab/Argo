{
    "data": {
        "repository": {
            "issues": {
                "totalCount": 711,
                "nodes": [
                    {
                        "title": "Add support for input masks in evasion attacks where possible",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/652",
                        "createdAt": "2020-10-09T11:36:01Z",
                        "closedAt": "2020-12-01T12:16:50Z"
                    },
                    {
                        "title": "Enable Consistent Outputs From TF/Pytorch Faster-RCNN's",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/655",
                        "createdAt": "2020-10-13T19:28:22Z",
                        "closedAt": "2020-11-04T14:46:14Z"
                    },
                    {
                        "title": "Check if preprocessing_defences can be enabled in PyTorchDeepSpeech",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/657",
                        "createdAt": "2020-10-14T11:32:49Z",
                        "closedAt": "2020-11-04T14:46:25Z"
                    },
                    {
                        "title": "Enable PGD and FGSM attacks on `art.estimators.speech_recognition` estimators",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/658",
                        "createdAt": "2020-10-14T12:53:40Z",
                        "closedAt": "2020-11-04T14:46:37Z"
                    },
                    {
                        "title": "Enable passing a list for DPatch's patch_shape",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/661",
                        "createdAt": "2020-10-14T20:32:33Z",
                        "closedAt": "2020-11-04T14:49:00Z"
                    },
                    {
                        "title": "Implement STRIP defense against poisoning attacks",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/664",
                        "createdAt": "2020-10-15T19:17:37Z",
                        "closedAt": "2020-11-14T04:28:16Z"
                    },
                    {
                        "title": "Add backward compatibility for TensorFlow v2 to v1",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/665",
                        "createdAt": "2020-10-15T19:20:28Z",
                        "closedAt": "2021-03-16T17:56:41Z"
                    },
                    {
                        "title": "Add clean label backdoor attack  ",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/666",
                        "createdAt": "2020-10-15T19:20:42Z",
                        "closedAt": "2020-11-20T16:49:44Z"
                    },
                    {
                        "title": "PyTorchDeepSpeech.loss_gradient seems to fail with AttributeError in line 412",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/668",
                        "createdAt": "2020-10-15T23:35:46Z",
                        "closedAt": "2020-10-16T00:20:34Z"
                    },
                    {
                        "title": "FGSM attack not working well (cannot replicate adversarial_training_mnist.ipynb under notebooks)",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/670",
                        "createdAt": "2020-10-16T16:39:38Z",
                        "closedAt": "2020-12-01T12:27:50Z"
                    },
                    {
                        "title": "Add Safe Predictions for Abstaining Classifiers",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/671",
                        "createdAt": "2020-10-16T17:27:03Z",
                        "closedAt": null
                    },
                    {
                        "title": "NaN values in adversarial examples generated by ImperceptibleASRPytorch",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/672",
                        "createdAt": "2020-10-16T23:46:33Z",
                        "closedAt": "2020-11-04T14:51:20Z"
                    },
                    {
                        "title": "Enhance JPegCompression defense",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/673",
                        "createdAt": "2020-10-20T14:12:45Z",
                        "closedAt": "2020-12-01T12:16:46Z"
                    },
                    {
                        "title": "Enable passing a list for attacks' mask argument",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/674",
                        "createdAt": "2020-10-20T14:42:16Z",
                        "closedAt": "2020-12-01T12:27:34Z"
                    },
                    {
                        "title": "Using Multiple Attacks ",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/675",
                        "createdAt": "2020-10-20T15:21:01Z",
                        "closedAt": "2020-10-20T15:23:27Z"
                    },
                    {
                        "title": "Support variable length inputs in preprocessing defenses",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/680",
                        "createdAt": "2020-10-21T12:46:02Z",
                        "closedAt": "2020-12-01T12:16:12Z"
                    },
                    {
                        "title": "What is the adversarial training approach in classifier?",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/683",
                        "createdAt": "2020-10-22T01:35:34Z",
                        "closedAt": "2020-10-23T18:03:39Z"
                    },
                    {
                        "title": "classifier accuracy increasing as attack strength (epsilon) increases",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/687",
                        "createdAt": "2020-10-26T21:05:46Z",
                        "closedAt": "2020-11-09T16:58:35Z"
                    },
                    {
                        "title": "PGD and FGM fail on PyTorchDeepSpeech if labels are not provided",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/688",
                        "createdAt": "2020-10-27T17:37:21Z",
                        "closedAt": "2020-11-04T14:48:32Z"
                    },
                    {
                        "title": "Add progress bar to art.utils.get_file",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/692",
                        "createdAt": "2020-10-28T18:00:20Z",
                        "closedAt": "2020-12-01T12:15:50Z"
                    },
                    {
                        "title": "Make ART_DATA_PATH configurable after import",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/693",
                        "createdAt": "2020-10-28T21:12:18Z",
                        "closedAt": "2020-12-01T12:16:41Z"
                    },
                    {
                        "title": "Making numpy object arrays from ragged sequences is deprecated",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/694",
                        "createdAt": "2020-10-28T22:20:54Z",
                        "closedAt": "2020-11-04T14:48:48Z"
                    },
                    {
                        "title": "Enable PGD attack on PyTorch Faster-RCNN using np object arrays as input",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/697",
                        "createdAt": "2020-10-29T21:40:58Z",
                        "closedAt": null
                    },
                    {
                        "title": "Certified accuracy",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/699",
                        "createdAt": "2020-10-29T22:42:20Z",
                        "closedAt": "2020-11-02T20:45:46Z"
                    },
                    {
                        "title": "unable to replicate a result from Athalye et al 2018 with art -- issue with gradient calculation?",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/702",
                        "createdAt": "2020-10-30T21:24:26Z",
                        "closedAt": "2020-11-05T21:35:39Z"
                    },
                    {
                        "title": "Add a small number of test audio samples",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/706",
                        "createdAt": "2020-11-02T14:34:18Z",
                        "closedAt": null
                    },
                    {
                        "title": "Add test runner for Lingvo ASR estimator",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/707",
                        "createdAt": "2020-11-02T14:51:23Z",
                        "closedAt": "2021-03-16T21:35:08Z"
                    },
                    {
                        "title": "Simpler usage for PreprocessorPyTorch",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/708",
                        "createdAt": "2020-11-03T01:46:02Z",
                        "closedAt": "2020-12-01T12:16:02Z"
                    },
                    {
                        "title": "Change application of mask in PGD",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/710",
                        "createdAt": "2020-11-04T11:33:50Z",
                        "closedAt": "2020-12-01T12:16:08Z"
                    },
                    {
                        "title": "Change Implementation of Random Restarts in PGD ",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/714",
                        "createdAt": "2020-11-05T14:45:57Z",
                        "closedAt": "2020-12-01T12:16:25Z"
                    },
                    {
                        "title": "Attacked image generated to fool tesseract not working ",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/715",
                        "createdAt": "2020-11-05T19:28:32Z",
                        "closedAt": "2020-12-01T12:28:43Z"
                    },
                    {
                        "title": "Remove all of the duplicated and slightly different fix_get_mnist_subset fixture",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/717",
                        "createdAt": "2020-11-06T15:34:44Z",
                        "closedAt": null
                    },
                    {
                        "title": "data augmentation with classifier.fit",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/721",
                        "createdAt": "2020-11-08T14:13:52Z",
                        "closedAt": "2020-11-10T20:55:35Z"
                    },
                    {
                        "title": "Questions regarding adversarial trainer",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/724",
                        "createdAt": "2020-11-09T17:37:04Z",
                        "closedAt": "2021-07-12T16:16:47Z"
                    },
                    {
                        "title": "PyTorchDataGenerator always returns the same batch",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/728",
                        "createdAt": "2020-11-11T00:25:42Z",
                        "closedAt": "2020-11-21T01:00:26Z"
                    },
                    {
                        "title": "Github Action per Framework",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/729",
                        "createdAt": "2020-11-11T14:15:31Z",
                        "closedAt": "2020-12-01T12:29:12Z"
                    },
                    {
                        "title": "Remove @FrameworkAgnostic Markers",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/732",
                        "createdAt": "2020-11-12T10:55:56Z",
                        "closedAt": "2021-04-16T23:58:57Z"
                    },
                    {
                        "title": "Test run analysis",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/733",
                        "createdAt": "2020-11-12T10:57:11Z",
                        "closedAt": "2021-04-16T23:59:02Z"
                    },
                    {
                        "title": "Support for Python > 3.7 and/or scikit-learn > 0.22",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/734",
                        "createdAt": "2020-11-13T00:22:29Z",
                        "closedAt": "2020-12-01T12:16:34Z"
                    },
                    {
                        "title": "Randomized Smoothing Wrapper",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/735",
                        "createdAt": "2020-11-13T18:19:17Z",
                        "closedAt": "2020-12-01T12:26:56Z"
                    },
                    {
                        "title": "example of using ExpectationOverTransformation (EoT) wrapper with preprocessing defence",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/739",
                        "createdAt": "2020-11-16T20:38:16Z",
                        "closedAt": "2020-12-03T15:17:19Z"
                    },
                    {
                        "title": "empirical robustness",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/742",
                        "createdAt": "2020-11-17T07:42:24Z",
                        "closedAt": "2021-04-16T23:59:06Z"
                    },
                    {
                        "title": "Investigate Pytorch compatibility with poisoning attacks",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/745",
                        "createdAt": "2020-11-17T16:59:41Z",
                        "closedAt": null
                    },
                    {
                        "title": "Move all poisoning defense tests to the new ART Testing Framework",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/746",
                        "createdAt": "2020-11-17T17:01:51Z",
                        "closedAt": null
                    },
                    {
                        "title": "Support preprocessing in the pytorch imperceptable asr attack",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/747",
                        "createdAt": "2020-11-17T19:34:28Z",
                        "closedAt": "2020-12-01T12:28:16Z"
                    },
                    {
                        "title": "Any example/tutorial that demonstrate the model extraction attacks?",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/756",
                        "createdAt": "2020-11-24T02:09:54Z",
                        "closedAt": "2021-01-08T15:06:23Z"
                    },
                    {
                        "title": "A Little Suggestion: Add noise return for UniversalPerturbation",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/758",
                        "createdAt": "2020-11-25T14:54:10Z",
                        "closedAt": "2020-11-26T01:26:50Z"
                    },
                    {
                        "title": "\"Only Keras classifiers (v2.2.4) are supported for this defence\" in poisoning_defense_neural_cleanse.ipynd",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/764",
                        "createdAt": "2020-11-27T00:45:34Z",
                        "closedAt": null
                    },
                    {
                        "title": "BinaryInputDetector Discussion",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/766",
                        "createdAt": "2020-11-28T00:47:15Z",
                        "closedAt": "2021-04-16T23:59:20Z"
                    },
                    {
                        "title": "max_iter in PGD attack does not work",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/769",
                        "createdAt": "2020-11-29T12:53:16Z",
                        "closedAt": "2020-11-29T12:59:28Z"
                    },
                    {
                        "title": "detection_adversarial_samples_cifar10.ipynb is not working correctly on nbviewer. The reason is tf version.",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/770",
                        "createdAt": "2020-11-30T03:33:04Z",
                        "closedAt": null
                    },
                    {
                        "title": "[install failed] UnicodeDecodeError: 'gbk' codec can't decode byte 0xad in position 1843: illegal multibyte sequence",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/771",
                        "createdAt": "2020-11-30T08:20:25Z",
                        "closedAt": "2020-12-01T12:17:16Z"
                    },
                    {
                        "title": "\"Divided by zero issue\" using Wasserstein Attack",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/776",
                        "createdAt": "2020-12-01T01:45:18Z",
                        "closedAt": "2021-01-09T02:18:21Z"
                    },
                    {
                        "title": "TypeError: __init__() got an unexpected keyword argument 'max_iters' in PoisoningAttackSVM.ipynb",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/777",
                        "createdAt": "2020-12-01T18:26:06Z",
                        "closedAt": "2020-12-01T22:16:41Z"
                    },
                    {
                        "title": "Implement set_learning_phase method(s) as no-op where applicable",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/779",
                        "createdAt": "2020-12-02T00:32:05Z",
                        "closedAt": "2021-03-16T17:56:46Z"
                    },
                    {
                        "title": "Argument model not defined in super.__init__() of PyTorchFasterRCNN",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/788",
                        "createdAt": "2020-12-08T10:54:57Z",
                        "closedAt": "2021-01-09T02:18:41Z"
                    },
                    {
                        "title": "Rename skipMlFramework to skip_framework or skip_ml_framework",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/793",
                        "createdAt": "2020-12-09T17:35:33Z",
                        "closedAt": "2021-03-16T17:56:43Z"
                    },
                    {
                        "title": "What is the fastest way to create adverserial examples for RandomForest ou GradientBoosting? Are there ways to speed up the process?",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/795",
                        "createdAt": "2020-12-10T21:56:17Z",
                        "closedAt": "2021-04-16T23:57:57Z"
                    },
                    {
                        "title": "Example use of Some of the attacks",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/797",
                        "createdAt": "2020-12-13T00:16:27Z",
                        "closedAt": null
                    },
                    {
                        "title": "Use tqdm.auto imports for use in notebooks",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/798",
                        "createdAt": "2020-12-13T22:23:29Z",
                        "closedAt": "2021-01-09T02:18:49Z"
                    },
                    {
                        "title": "Having trouble generating speech recognition adversarial samples.",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/800",
                        "createdAt": "2020-12-15T18:13:21Z",
                        "closedAt": "2021-01-09T02:20:56Z"
                    },
                    {
                        "title": "PixelThreshold attack does not scale back input samples in range [0, 1]",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/801",
                        "createdAt": "2020-12-15T20:07:58Z",
                        "closedAt": "2021-01-09T02:18:53Z"
                    },
                    {
                        "title": "Github Action which verifies that each ART test was run at least once",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/803",
                        "createdAt": "2020-12-17T13:58:11Z",
                        "closedAt": "2020-12-17T14:34:08Z"
                    },
                    {
                        "title": "Integrate all duplicated mnist_subset fixtures across tests",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/804",
                        "createdAt": "2020-12-17T14:00:29Z",
                        "closedAt": "2020-12-17T14:35:17Z"
                    },
                    {
                        "title": "Update KerasClassifier for deprecation of set_learning_phase",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/806",
                        "createdAt": "2020-12-18T23:30:31Z",
                        "closedAt": "2021-02-05T17:42:42Z"
                    },
                    {
                        "title": "Remove @FrameworkAgnostic Markers",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/807",
                        "createdAt": "2020-12-22T13:00:58Z",
                        "closedAt": null
                    },
                    {
                        "title": "Missing Keras import guard in KerasClassifier's custom_loss_gradient function.",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/809",
                        "createdAt": "2020-12-23T09:20:28Z",
                        "closedAt": "2021-01-09T02:17:50Z"
                    },
                    {
                        "title": "UnboundLocalError: local variable 'archive' referenced before assignment????",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/811",
                        "createdAt": "2020-12-25T14:06:05Z",
                        "closedAt": "2021-03-16T18:00:55Z"
                    },
                    {
                        "title": "Can we use TF Data API with art.estimators.classification.KerasClassifier?",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/812",
                        "createdAt": "2021-01-01T12:45:50Z",
                        "closedAt": "2021-01-01T12:50:03Z"
                    },
                    {
                        "title": "Support for multioutput model ",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/813",
                        "createdAt": "2021-01-02T19:54:06Z",
                        "closedAt": null
                    },
                    {
                        "title": "Successful Pixel Attack and Threshold Attacks!",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/817",
                        "createdAt": "2021-01-04T20:55:50Z",
                        "closedAt": "2021-01-09T02:20:29Z"
                    },
                    {
                        "title": "Add support for Numba and investigate performance improvements",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/819",
                        "createdAt": "2021-01-05T13:16:29Z",
                        "closedAt": "2021-03-16T17:56:48Z"
                    },
                    {
                        "title": "Add subclass-specific params to estimator_params list",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/821",
                        "createdAt": "2021-01-06T14:23:56Z",
                        "closedAt": "2021-03-16T17:56:51Z"
                    },
                    {
                        "title": "Expand range of eps_step (and potentially eps) in PGD",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/823",
                        "createdAt": "2021-01-07T00:41:48Z",
                        "closedAt": "2021-03-16T17:57:15Z"
                    },
                    {
                        "title": "Better PGD handling of NaN and Inf gradient outputs",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/824",
                        "createdAt": "2021-01-07T00:58:07Z",
                        "closedAt": "2021-03-16T17:57:13Z"
                    },
                    {
                        "title": "SklearnClassifier produces probabilities instead of logits for DeepFool",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/826",
                        "createdAt": "2021-01-07T22:19:56Z",
                        "closedAt": "2021-03-16T17:57:10Z"
                    },
                    {
                        "title": "Package name \"art\" has already been taken on pypi",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/830",
                        "createdAt": "2021-01-08T23:28:47Z",
                        "closedAt": null
                    },
                    {
                        "title": "Implement Convex Polytope clean label attack ",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/831",
                        "createdAt": "2021-01-08T23:51:53Z",
                        "closedAt": "2021-03-16T17:58:04Z"
                    },
                    {
                        "title": "PGD fails when running targeted attack with PyTorch or TensorFlow",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/832",
                        "createdAt": "2021-01-09T16:33:21Z",
                        "closedAt": "2021-02-20T01:13:41Z"
                    },
                    {
                        "title": "AutoAttack should pass kwargs of generate to internal calls attack.generate",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/835",
                        "createdAt": "2021-01-11T22:50:48Z",
                        "closedAt": "2021-02-20T01:13:45Z"
                    },
                    {
                        "title": "SklearnClassifier does not support sklearn-like estimators (e.g. ThunderSVM)",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/836",
                        "createdAt": "2021-01-11T22:53:35Z",
                        "closedAt": null
                    },
                    {
                        "title": "'BlackBoxClassifier' object has no attribute 'channels_first'",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/837",
                        "createdAt": "2021-01-13T01:52:19Z",
                        "closedAt": "2021-02-20T01:13:50Z"
                    },
                    {
                        "title": "Bad performance of targeted versions of AutoPGD and PGD attacks ",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/840",
                        "createdAt": "2021-01-14T13:58:21Z",
                        "closedAt": "2021-02-20T01:14:21Z"
                    },
                    {
                        "title": "JSMA only iterates once when clip_values is not set",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/841",
                        "createdAt": "2021-01-14T15:53:10Z",
                        "closedAt": "2021-02-20T01:13:55Z"
                    },
                    {
                        "title": "JSMA only iterates one when clip_values is not set",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/842",
                        "createdAt": "2021-01-14T15:54:07Z",
                        "closedAt": "2021-01-14T15:55:20Z"
                    },
                    {
                        "title": "two questions on zoo attack.",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/845",
                        "createdAt": "2021-01-15T15:16:11Z",
                        "closedAt": "2021-04-16T23:57:31Z"
                    },
                    {
                        "title": "Preprocessing for PyTorch models",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/846",
                        "createdAt": "2021-01-18T07:57:02Z",
                        "closedAt": "2021-03-16T21:39:47Z"
                    },
                    {
                        "title": "Update channel index selection in ShadowAttack for PyTorch models",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/847",
                        "createdAt": "2021-01-18T13:39:39Z",
                        "closedAt": "2021-02-20T01:14:11Z"
                    },
                    {
                        "title": "Implement Adversarial Patch framework-specific in PyTorch",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/851",
                        "createdAt": "2021-01-18T19:33:03Z",
                        "closedAt": "2021-03-16T17:58:10Z"
                    },
                    {
                        "title": "Add EoT for image modifications like brightness, contrast, etc.",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/852",
                        "createdAt": "2021-01-18T20:18:31Z",
                        "closedAt": "2021-03-16T17:58:12Z"
                    },
                    {
                        "title": "Extend mask feature in AdversarialPatch*.apply_patch to include perspective transformation",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/853",
                        "createdAt": "2021-01-18T20:27:19Z",
                        "closedAt": "2021-03-16T17:58:28Z"
                    },
                    {
                        "title": "Add patch reset option to method generate of Adversarial Patch attacks",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/854",
                        "createdAt": "2021-01-18T20:32:05Z",
                        "closedAt": "2021-02-20T01:14:15Z"
                    },
                    {
                        "title": "Batch SimBA",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/855",
                        "createdAt": "2021-01-19T03:58:04Z",
                        "closedAt": "2021-02-20T01:15:50Z"
                    },
                    {
                        "title": "SimBA bug? inconsistent with the original paper ",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/856",
                        "createdAt": "2021-01-19T07:07:25Z",
                        "closedAt": "2021-01-19T11:23:20Z"
                    },
                    {
                        "title": "Cross Evaluation Test Parent Test Class",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/859",
                        "createdAt": "2021-01-21T11:08:13Z",
                        "closedAt": null
                    },
                    {
                        "title": "Enable GPU support in PyTorchClassifier.loss",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/861",
                        "createdAt": "2021-01-22T21:41:28Z",
                        "closedAt": "2021-02-20T01:14:26Z"
                    },
                    {
                        "title": "Support for TensorBoard in art.attacks",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/864",
                        "createdAt": "2021-01-26T13:12:21Z",
                        "closedAt": "2021-06-15T22:19:25Z"
                    },
                    {
                        "title": "New preprocessing model for natural corruptions",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/865",
                        "createdAt": "2021-01-26T13:28:34Z",
                        "closedAt": "2021-05-24T20:38:59Z"
                    },
                    {
                        "title": "Implement estimators and attacks for NLP tasks",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/866",
                        "createdAt": "2021-01-26T13:36:26Z",
                        "closedAt": null
                    },
                    {
                        "title": "Functionally Equivalent Extraction delta converges to 0",
                        "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/issues/867",
                        "createdAt": "2021-01-26T14:22:31Z",
                        "closedAt": "2021-02-20T01:16:14Z"
                    }
                ],
                "pageInfo": {
                    "endCursor": "Y3Vyc29yOnYyOpHOL1eb2g==",
                    "hasNextPage": true
                }
            }
        }
    }
}